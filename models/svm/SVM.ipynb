{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "024f6e96",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVMs)\n",
    "The original SVM algorithm was invented by Vladimir N. Vapnik and Alexey Ya. Chervonenkis in 1964. In 1992, Bernhard Boser, Isabelle Guyon and Vladimir Vapnik suggested a way to create nonlinear classifiers by applying the kernel trick to maximum-margin hyperplanes. The \"soft margin\" incarnation, as is commonly used in software packages, was proposed by Corinna Cortes and Vapnik in 1993 and published in 1995.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"./../assets/svm.png\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "> Image from acte.in\n",
    "\n",
    "## Logic behind SVMs\n",
    "A support vector machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It works by finding the best hyperplane that separates different classes in the data, maximizing the margin between them.\n",
    "\n",
    "### What are Support Vectors?\n",
    "**Support vectors**: are the data points that lie closest to the decision boundary (hyperplane) in an SVM model. These points are crucial because they define the margin of the classifier.\n",
    "\n",
    "**Margin**: is the distance between the hyperplane and the nearest data points from either class.\n",
    "\n",
    "The goal of SVM is to maximize this margin, thereby creating a robust classifier that generalizes well to unseen data.\n",
    "\n",
    "### The Role of Training Data\n",
    "The amount and quality of training data significantly impact the number of support vectors and, consequently, the performance of the SVM classifier. Here’s how:\n",
    "1. **Data Complexity**:  If the training data is complex and not easily separable, the SVM will require more support vectors to define the decision boundary. For instance, in a high-dimensional space with intricate patterns, more support vectors are needed to capture the nuances of the data distribution.\n",
    "\n",
    "2. **Sample Size**: The number of training samples directly influences the number of support vectors. In scenarios where the training set is large, the SVM might end up using a substantial portion of the data as support vectors, especially if the data is noisy or not well-separated. Conversely, with a smaller training set, fewer support vectors might be sufficient, but this can lead to overfitting if the model becomes too sensitive to the limited data\n",
    "\n",
    "3. **Feature Space**: The dimensionality of the feature space also plays a role. Higher-dimensional spaces can lead to more complex decision boundaries, requiring more support vectors. However, this also increases the risk of overfitting, where the model captures noise rather than the underlying pattern.\n",
    "\n",
    "\n",
    "### Impact of Support Vectors on Classifier Performance\n",
    "The number of support vectors has a direct impact on both the accuracy and computational efficiency of the SVM classifier.\n",
    "\n",
    "#### Accuracy:\n",
    "1. **Generlaization**: A model with too many support vectors might indicate overfitting, where the classifier performs well on training data but poorly on unseen data. This is because the model is too complex and captures noise in the training data.\n",
    "\n",
    "2. **Underfitting**: Conversely, too few support vectors might lead to underfitting, where the model is too simplistic to capture the underlying patterns in the data, resulting in poor performance on both training and test data.\n",
    "\n",
    "3. **Trade-off**: *The trade-off between margin maximization and classification error is controlled by the regularization parameter C*. A high C value allows fewer misclassifications but can lead to more support vectors and a complex model. A lower C value increases the margin but allows more misclassifications, potentially reducing the number of support vectors and simplifying the model.\n",
    "\n",
    "#### Computional Complexity\n",
    "1. **Training Time**: The training time of an SVM is influenced by the number of support vectors. More support vectors mean more computations during the training phase, as the algorithm needs to solve a larger optimization problem.\n",
    "\n",
    "2. **Prediction Time**: During prediction, the SVM classifier computes the dot product between the test point and each support vector. Hence, a larger number of support vectors increases the prediction time, making the model less efficient for real-time applications.\n",
    "\n",
    "#### Optimizing SVM Performance: Practical Considerations\n",
    "1. **Kernel Choice**: The choice of kernel function (linear, polynomial, radial basis function, etc.) affects the number of support vectors. Non-linear kernels, while powerful, often result in more support vectors due to their ability to capture complex patterns in the data.\n",
    "\n",
    "2. **Parameter Tuning**: Proper tuning of SVM parameters, such as the regularization parameter C and kernel parameters, is crucial. Techniques like cross-validation can help in finding the optimal parameters that balance the number of support vectors and classifier performance.\n",
    "\n",
    "3. **Data Preprocessing**: Preprocessing steps like normalization, feature selection, and dimensionality reduction can reduce the complexity of the data, potentially decreasing the number of support vectors needed and improving the model’s performance.\n",
    "\n",
    "<a href=\"https://www.geeksforgeeks.org/machine-learning/optimizing-svm-classifiers-the-role-of-support-vectors-in-training-data-and-performance/\">Notes from geeksforgeeks</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
